# -*- coding: utf-8 -*-
"""codtech_Task4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YRsQEt-FbWx7NGpNwXJmveSv8-VWFN8U
"""

# Movie Recommendation System using Collaborative Filtering and Matrix Factorization
# Ready for Google Colab

# Install required packages
!pip install pandas numpy matplotlib seaborn scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Import sklearn components with error handling
try:
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, mean_absolute_error
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import NMF
    SKLEARN_AVAILABLE = True
    print("✅ scikit-learn imported successfully")
except ImportError as e:
    print(f"⚠️ scikit-learn import error: {e}")
    print("Installing scikit-learn...")
    !pip install scikit-learn

    # Try importing again
    try:
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error, mean_absolute_error
        from sklearn.metrics.pairwise import cosine_similarity
        from sklearn.decomposition import NMF
        SKLEARN_AVAILABLE = True
        print("✅ scikit-learn imported successfully after installation")
    except ImportError:
        print("❌ Could not import scikit-learn. Using alternative implementations.")
        SKLEARN_AVAILABLE = False

# Alternative implementations if sklearn is not available
if not SKLEARN_AVAILABLE:
    def train_test_split(data, test_size=0.2, random_state=42):
        """Alternative train_test_split implementation"""
        np.random.seed(random_state)
        n_test = int(len(data) * test_size)

        # Shuffle the dataframe
        data_shuffled = data.sample(frac=1, random_state=random_state).reset_index(drop=True)

        # Split the data
        test_data = data_shuffled.iloc[:n_test].copy()
        train_data = data_shuffled.iloc[n_test:].copy()

        return train_data, test_data

    def mean_squared_error(y_true, y_pred):
        """Alternative MSE implementation"""
        return np.mean((np.array(y_true) - np.array(y_pred)) ** 2)

    def mean_absolute_error(y_true, y_pred):
        """Alternative MAE implementation"""
        return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))

    def cosine_similarity(X):
        """Alternative cosine similarity implementation"""
        # Normalize rows
        X_normalized = X / np.linalg.norm(X, axis=1, keepdims=True)
        X_normalized = np.nan_to_num(X_normalized)  # Handle division by zero
        return np.dot(X_normalized, X_normalized.T)

    class NMF:
        """Alternative NMF implementation using basic matrix factorization"""
        def __init__(self, n_components=50, random_state=42, max_iter=200):
            self.n_components = n_components
            self.random_state = random_state
            self.max_iter = max_iter
            self.components_ = None

        def fit_transform(self, X):
            np.random.seed(self.random_state)
            n_samples, n_features = X.shape

            # Initialize factors randomly
            W = np.random.uniform(0, 1, (n_samples, self.n_components))
            H = np.random.uniform(0, 1, (self.n_components, n_features))

            # Simple alternating least squares
            for _ in range(min(50, self.max_iter)):  # Reduced iterations for speed
                # Update H
                for j in range(n_features):
                    if np.sum(X[:, j]) > 0:  # Only update if column has non-zero values
                        H[:, j] = np.maximum(0.01, np.linalg.lstsq(W, X[:, j], rcond=None)[0])

                # Update W
                for i in range(n_samples):
                    if np.sum(X[i, :]) > 0:  # Only update if row has non-zero values
                        W[i, :] = np.maximum(0.01, np.linalg.lstsq(H.T, X[i, :], rcond=None)[0])

            self.components_ = H
            return W

    print("✅ Alternative implementations loaded")

# Set random seed for reproducibility
np.random.seed(42)

print("🎬 Movie Recommendation System")
print("=" * 50)

# Generate synthetic movie rating data
def generate_movie_data(n_users=1000, n_movies=500, n_ratings=50000):
    """Generate synthetic movie rating dataset"""

    # Create movie metadata
    genres = ['Action', 'Comedy', 'Drama', 'Horror', 'Romance', 'Sci-Fi', 'Thriller']
    movies = pd.DataFrame({
        'movie_id': range(1, n_movies + 1),
        'title': [f'Movie_{i}' for i in range(1, n_movies + 1)],
        'genre': np.random.choice(genres, n_movies),
        'year': np.random.randint(1990, 2024, n_movies)
    })

    # Generate ratings with some patterns (users tend to rate movies in similar genres)
    ratings_data = []

    for _ in range(n_ratings):
        user_id = np.random.randint(1, n_users + 1)
        movie_id = np.random.randint(1, n_movies + 1)

        # Add some bias based on genre preferences
        movie_genre = movies[movies['movie_id'] == movie_id]['genre'].iloc[0]
        base_rating = np.random.normal(3.5, 1.2)

        # Users have preferences for certain genres
        user_genre_bias = np.random.normal(0, 0.5) if hash(f"{user_id}_{movie_genre}") % 3 == 0 else 0
        rating = base_rating + user_genre_bias

        # Clip rating to 1-5 scale
        rating = max(1, min(5, rating))

        ratings_data.append({
            'user_id': user_id,
            'movie_id': movie_id,
            'rating': round(rating, 1)
        })

    ratings = pd.DataFrame(ratings_data)

    # Remove duplicates (same user rating same movie multiple times)
    ratings = ratings.drop_duplicates(subset=['user_id', 'movie_id'])

    return ratings, movies

# Generate the dataset
print("📊 Generating synthetic movie rating dataset...")
ratings_df, movies_df = generate_movie_data()

print(f"Dataset Statistics:")
print(f"- Users: {ratings_df['user_id'].nunique()}")
print(f"- Movies: {ratings_df['movie_id'].nunique()}")
print(f"- Ratings: {len(ratings_df)}")
print(f"- Sparsity: {(1 - len(ratings_df) / (ratings_df['user_id'].nunique() * ratings_df['movie_id'].nunique())) * 100:.2f}%")

# Data exploration
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
ratings_df['rating'].hist(bins=20, alpha=0.7, color='skyblue')
plt.title('Rating Distribution')
plt.xlabel('Rating')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
user_counts = ratings_df['user_id'].value_counts()
plt.hist(user_counts, bins=30, alpha=0.7, color='lightgreen')
plt.title('Ratings per User')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Users')

plt.subplot(1, 3, 3)
movie_counts = ratings_df['movie_id'].value_counts()
plt.hist(movie_counts, bins=30, alpha=0.7, color='salmon')
plt.title('Ratings per Movie')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Movies')

plt.tight_layout()
plt.show()

# Split data into train/test
train_data, test_data = train_test_split(ratings_df, test_size=0.2, random_state=42)
print(f"\n📈 Data Split:")
print(f"- Training samples: {len(train_data)}")
print(f"- Test samples: {len(test_data)}")

# Create user-item matrix
def create_user_item_matrix(data):
    """Create user-item rating matrix"""
    return data.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)

user_item_matrix = create_user_item_matrix(train_data)
print(f"\n🔢 User-Item Matrix Shape: {user_item_matrix.shape}")

# ==========================================
# 1. USER-BASED COLLABORATIVE FILTERING
# ==========================================

class UserBasedCF:
    def __init__(self, user_item_matrix, k=20):
        self.user_item_matrix = user_item_matrix
        self.k = k  # number of similar users to consider
        self.user_similarity = None

    def fit(self):
        """Calculate user-user similarity matrix"""
        print("🔄 Computing user similarities...")

        # Calculate cosine similarity between users
        user_matrix = self.user_item_matrix.values
        self.user_similarity = cosine_similarity(user_matrix)

        # Convert to DataFrame for easier handling
        self.user_similarity = pd.DataFrame(
            self.user_similarity,
            index=self.user_item_matrix.index,
            columns=self.user_item_matrix.index
        )

    def predict(self, user_id, movie_id):
        """Predict rating for a user-movie pair"""
        if user_id not in self.user_item_matrix.index:
            return self.user_item_matrix.mean().mean()  # Global average

        if movie_id not in self.user_item_matrix.columns:
            return self.user_item_matrix.loc[user_id].mean()  # User average

        # Get users who rated this movie
        movie_ratings = self.user_item_matrix[movie_id]
        rated_users = movie_ratings[movie_ratings > 0].index

        if len(rated_users) == 0:
            return self.user_item_matrix.loc[user_id].mean()

        # Get similarities with users who rated this movie
        similarities = self.user_similarity.loc[user_id, rated_users]

        # Get top-k similar users
        top_users = similarities.nlargest(self.k).index

        if len(top_users) == 0:
            return self.user_item_matrix.loc[user_id].mean()

        # Calculate weighted average
        numerator = sum(similarities[u] * movie_ratings[u] for u in top_users)
        denominator = sum(abs(similarities[u]) for u in top_users)

        if denominator == 0:
            return self.user_item_matrix.loc[user_id].mean()

        return numerator / denominator

    def recommend(self, user_id, n_recommendations=10):
        """Recommend top-N movies for a user"""
        if user_id not in self.user_item_matrix.index:
            return []

        # Get movies not rated by the user
        user_ratings = self.user_item_matrix.loc[user_id]
        unrated_movies = user_ratings[user_ratings == 0].index

        # Predict ratings for unrated movies
        predictions = []
        for movie_id in unrated_movies:
            pred_rating = self.predict(user_id, movie_id)
            predictions.append((movie_id, pred_rating))

        # Sort by predicted rating and return top-N
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:n_recommendations]

# ==========================================
# 2. ITEM-BASED COLLABORATIVE FILTERING
# ==========================================

class ItemBasedCF:
    def __init__(self, user_item_matrix, k=20):
        self.user_item_matrix = user_item_matrix
        self.k = k
        self.item_similarity = None

    def fit(self):
        """Calculate item-item similarity matrix"""
        print("🔄 Computing item similarities...")

        # Transpose to get item-user matrix
        item_matrix = self.user_item_matrix.T.values
        self.item_similarity = cosine_similarity(item_matrix)

        # Convert to DataFrame
        self.item_similarity = pd.DataFrame(
            self.item_similarity,
            index=self.user_item_matrix.columns,
            columns=self.user_item_matrix.columns
        )

    def predict(self, user_id, movie_id):
        """Predict rating for a user-movie pair"""
        if user_id not in self.user_item_matrix.index:
            return self.user_item_matrix.mean().mean()

        if movie_id not in self.user_item_matrix.columns:
            return self.user_item_matrix.loc[user_id].mean()

        # Get movies rated by this user
        user_ratings = self.user_item_matrix.loc[user_id]
        rated_movies = user_ratings[user_ratings > 0].index

        if len(rated_movies) == 0:
            return self.user_item_matrix[movie_id].mean()

        # Get similarities with rated movies
        similarities = self.item_similarity.loc[movie_id, rated_movies]

        # Get top-k similar movies
        top_movies = similarities.nlargest(self.k).index

        if len(top_movies) == 0:
            return self.user_item_matrix[movie_id].mean()

        # Calculate weighted average
        numerator = sum(similarities[m] * user_ratings[m] for m in top_movies)
        denominator = sum(abs(similarities[m]) for m in top_movies)

        if denominator == 0:
            return self.user_item_matrix[movie_id].mean()

        return numerator / denominator

    def recommend(self, user_id, n_recommendations=10):
        """Recommend top-N movies for a user"""
        if user_id not in self.user_item_matrix.index:
            return []

        # Get movies not rated by the user
        user_ratings = self.user_item_matrix.loc[user_id]
        unrated_movies = user_ratings[user_ratings == 0].index

        # Predict ratings for unrated movies
        predictions = []
        for movie_id in unrated_movies:
            pred_rating = self.predict(user_id, movie_id)
            predictions.append((movie_id, pred_rating))

        # Sort by predicted rating and return top-N
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:n_recommendations]

# ==========================================
# 3. MATRIX FACTORIZATION (NMF)
# ==========================================

class MatrixFactorization:
    def __init__(self, user_item_matrix, n_components=50, random_state=42):
        self.user_item_matrix = user_item_matrix
        self.n_components = n_components
        self.random_state = random_state
        self.model = None
        self.user_features = None
        self.item_features = None

    def fit(self):
        """Fit NMF model"""
        print("🔄 Training Matrix Factorization model...")

        # NMF requires non-negative values
        matrix = self.user_item_matrix.values
        matrix = np.where(matrix < 0, 0, matrix)

        self.model = NMF(n_components=self.n_components, random_state=self.random_state, max_iter=200)
        self.user_features = self.model.fit_transform(matrix)
        self.item_features = self.model.components_

    def predict(self, user_id, movie_id):
        """Predict rating for a user-movie pair"""
        try:
            user_idx = list(self.user_item_matrix.index).index(user_id)
            movie_idx = list(self.user_item_matrix.columns).index(movie_id)

            prediction = np.dot(self.user_features[user_idx], self.item_features[:, movie_idx])
            return max(1, min(5, prediction))  # Clip to rating scale
        except (ValueError, IndexError):
            return self.user_item_matrix.mean().mean()

    def recommend(self, user_id, n_recommendations=10):
        """Recommend top-N movies for a user"""
        try:
            user_idx = list(self.user_item_matrix.index).index(user_id)

            # Get user's predicted ratings for all movies
            user_ratings = self.user_item_matrix.loc[user_id]
            predictions = []

            for movie_id in self.user_item_matrix.columns:
                if user_ratings[movie_id] == 0:  # Unrated movie
                    pred_rating = self.predict(user_id, movie_id)
                    predictions.append((movie_id, pred_rating))

            # Sort and return top-N
            predictions.sort(key=lambda x: x[1], reverse=True)
            return predictions[:n_recommendations]
        except ValueError:
            return []

# ==========================================
# TRAINING AND EVALUATION
# ==========================================

print("\n🚀 Training Recommendation Models...")

# Initialize models
user_cf = UserBasedCF(user_item_matrix, k=20)
item_cf = ItemBasedCF(user_item_matrix, k=20)
matrix_fact = MatrixFactorization(user_item_matrix, n_components=50)

# Train models
user_cf.fit()
item_cf.fit()
matrix_fact.fit()

print("✅ All models trained successfully!")

# Evaluation function
def evaluate_model(model, test_data, model_name):
    """Evaluate model performance"""
    print(f"\n📊 Evaluating {model_name}...")

    predictions = []
    actuals = []

    for _, row in test_data.sample(min(1000, len(test_data))).iterrows():  # Sample for faster evaluation
        user_id = row['user_id']
        movie_id = row['movie_id']
        actual_rating = row['rating']

        predicted_rating = model.predict(user_id, movie_id)

        predictions.append(predicted_rating)
        actuals.append(actual_rating)

    # Calculate metrics
    rmse = np.sqrt(mean_squared_error(actuals, predictions))
    mae = mean_absolute_error(actuals, predictions)

    return {
        'Model': model_name,
        'RMSE': rmse,
        'MAE': mae,
        'Predictions': len(predictions)
    }

# Evaluate all models
results = []
results.append(evaluate_model(user_cf, test_data, "User-Based CF"))
results.append(evaluate_model(item_cf, test_data, "Item-Based CF"))
results.append(evaluate_model(matrix_fact, test_data, "Matrix Factorization"))

# Display results
results_df = pd.DataFrame(results)
print("\n📈 Model Performance Comparison:")
print(results_df.to_string(index=False))

# Visualize results
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(results_df['Model'], results_df['RMSE'], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Root Mean Square Error (RMSE)')
plt.ylabel('RMSE')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
plt.bar(results_df['Model'], results_df['MAE'], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Mean Absolute Error (MAE)')
plt.ylabel('MAE')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# ==========================================
# RECOMMENDATION EXAMPLES
# ==========================================

print("\n🎯 Generating Sample Recommendations...")

# Select a random user for demonstration
sample_user = np.random.choice(user_item_matrix.index)
print(f"\n👤 Sample User ID: {sample_user}")

# Show user's rating history
user_history = train_data[train_data['user_id'] == sample_user].merge(movies_df, on='movie_id')
if len(user_history) > 0:
    print(f"\n📚 User's Rating History (showing top 10):")
    print(user_history.nlargest(10, 'rating')[['title', 'genre', 'rating']].to_string(index=False))

    # Get recommendations from each model
    print(f"\n🎬 Top 5 Recommendations for User {sample_user}:")
    print("-" * 60)

    # User-based CF recommendations
    user_recs = user_cf.recommend(sample_user, 5)
    print("👥 User-Based Collaborative Filtering:")
    for i, (movie_id, rating) in enumerate(user_recs, 1):
        movie_info = movies_df[movies_df['movie_id'] == movie_id].iloc[0]
        print(f"  {i}. {movie_info['title']} ({movie_info['genre']}) - Predicted: {rating:.2f}")

    # Item-based CF recommendations
    item_recs = item_cf.recommend(sample_user, 5)
    print("\n🎭 Item-Based Collaborative Filtering:")
    for i, (movie_id, rating) in enumerate(item_recs, 1):
        movie_info = movies_df[movies_df['movie_id'] == movie_id].iloc[0]
        print(f"  {i}. {movie_info['title']} ({movie_info['genre']}) - Predicted: {rating:.2f}")

    # Matrix factorization recommendations
    mf_recs = matrix_fact.recommend(sample_user, 5)
    print("\n🔢 Matrix Factorization:")
    for i, (movie_id, rating) in enumerate(mf_recs, 1):
        movie_info = movies_df[movies_df['movie_id'] == movie_id].iloc[0]
        print(f"  {i}. {movie_info['title']} ({movie_info['genre']}) - Predicted: {rating:.2f}")

# ==========================================
# ADDITIONAL ANALYSIS
# ==========================================

print("\n📊 Additional Analysis...")

# Genre-based analysis
genre_ratings = train_data.merge(movies_df, on='movie_id').groupby('genre')['rating'].agg(['mean', 'count'])
print("\n🎭 Average Ratings by Genre:")
print(genre_ratings.sort_values('mean', ascending=False))

# Visualize genre preferences
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
genre_ratings['mean'].plot(kind='bar', color='skyblue')
plt.title('Average Rating by Genre')
plt.ylabel('Average Rating')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
genre_ratings['count'].plot(kind='bar', color='lightcoral')
plt.title('Number of Ratings by Genre')
plt.ylabel('Number of Ratings')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Model complexity comparison
print("\n⚙️ Model Complexity Analysis:")
print(f"User-Based CF: O(n_users²) = O({user_item_matrix.shape[0]}²)")
print(f"Item-Based CF: O(n_items²) = O({user_item_matrix.shape[1]}²)")
print(f"Matrix Factorization: O(k*(n_users + n_items)) = O({matrix_fact.n_components}*{sum(user_item_matrix.shape)})")

print("\n🎉 Recommendation System Analysis Complete!")
print("\n📝 Summary:")
print("✓ Implemented User-Based Collaborative Filtering")
print("✓ Implemented Item-Based Collaborative Filtering")
print("✓ Implemented Matrix Factorization (NMF)")
print("✓ Evaluated models using RMSE and MAE")
print("✓ Generated personalized recommendations")
print("✓ Analyzed genre preferences and model complexity")

# Save results for future use
print(f"\n💾 Models and data are ready for further experimentation!")
print("You can now:")
print("- Generate recommendations for any user: user_cf.recommend(user_id, n)")
print("- Predict ratings: model.predict(user_id, movie_id)")
print("- Experiment with different parameters (k, n_components)")
print("- Add more sophisticated evaluation metrics")

